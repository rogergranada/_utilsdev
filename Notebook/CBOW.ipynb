{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Vocabulary Size:', 80)\n",
      "('Vocabulary Sample:', [('and', 18), ('our', 66), ('learning', 63), ('classification', 22), ('give', 40), ('predict', 29), ('skip', 74), ('is', 14), ('labels', 42), ('it', 15)])\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import text\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "norm_bible=[\"While the Word2Vec family of models are unsupervised, what this means is that you can just give it a corpus without additional labels or information and it can construct dense word embeddings from the corpus.\",\n",
    "            \"But you will still need to leverage a supervised, classification methodology once you have this corpus to get to these embeddings.\",\n",
    "            \"But we will do that from within the corpus itself, without any auxiliary information.\",\n",
    "            \" We can model this CBOW architecture now as a deep learning classification model such that we take in the context words as our input, X and try to predict the target word, Y. In fact building this architecture is simpler than the skip-gram model where we try to predict a whole bunch of context words from a source target word.\"\n",
    "           ]\n",
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(norm_bible)\n",
    "word2id = tokenizer.word_index\n",
    "\n",
    "# build vocabulary of unique words\n",
    "word2id['PAD'] = 0\n",
    "id2word = {v:k for k, v in word2id.items()}\n",
    "wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in norm_bible]\n",
    "\n",
    "vocab_size = len(word2id)\n",
    "embed_size = 100\n",
    "window_size = 2 # context window size\n",
    "\n",
    "print('Vocabulary Size:', vocab_size)\n",
    "print('Vocabulary Sample:', list(word2id.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Context (X):', ['while', 'the', 'family', 'of'], '-> Target (Y):', 'word2vec')\n",
      "('Context (X):', ['the', 'word2vec', 'of', 'models'], '-> Target (Y):', 'family')\n",
      "('Context (X):', ['word2vec', 'family', 'models', 'are'], '-> Target (Y):', 'of')\n",
      "('Context (X):', ['family', 'of', 'are', 'unsupervised'], '-> Target (Y):', 'models')\n",
      "('Context (X):', ['of', 'models', 'unsupervised', 'what'], '-> Target (Y):', 'are')\n",
      "('Context (X):', ['models', 'are', 'what', 'this'], '-> Target (Y):', 'unsupervised')\n",
      "('Context (X):', ['are', 'unsupervised', 'this', 'means'], '-> Target (Y):', 'what')\n",
      "('Context (X):', ['unsupervised', 'what', 'means', 'is'], '-> Target (Y):', 'this')\n",
      "('Context (X):', ['what', 'this', 'is', 'that'], '-> Target (Y):', 'means')\n",
      "('Context (X):', ['this', 'means', 'that', 'you'], '-> Target (Y):', 'is')\n",
      "('Context (X):', ['means', 'is', 'you', 'can'], '-> Target (Y):', 'that')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def generate_context_word_pairs(corpus, window_size, vocab_size):\n",
    "    context_length = window_size*2\n",
    "    for words in corpus:\n",
    "        sentence_length = len(words)\n",
    "        for index, word in enumerate(words):\n",
    "            context_words = []\n",
    "            label_word   = []            \n",
    "            start = index - window_size\n",
    "            end = index + window_size + 1\n",
    "            \n",
    "            context_words.append([words[i] \n",
    "                                 for i in range(start, end) \n",
    "                                 if 0 <= i < sentence_length \n",
    "                                 and i != index])\n",
    "            label_word.append(word)\n",
    "\n",
    "            x = sequence.pad_sequences(context_words, maxlen=context_length)\n",
    "            y = np_utils.to_categorical(label_word, vocab_size)\n",
    "            yield (x, y)\n",
    "            \n",
    "# Test this out for some samples\n",
    "i = 0\n",
    "for x, y in generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n",
    "    if 0 not in x[0]:\n",
    "        print('Context (X):', [id2word[w] for w in x[0]], '-> Target (Y):', id2word[np.argwhere(y[0])[0][0]])\n",
    "    \n",
    "        if i == 10:\n",
    "            break\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 4, 100)            8000      \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 80)                8080      \n",
      "=================================================================\n",
      "Total params: 16,080\n",
      "Trainable params: 16,080\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"308pt\" viewBox=\"0.00 0.00 238.00 308.00\" width=\"238pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 304)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-304 234,-304 234,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 139932875254608 -->\n",
       "<g class=\"node\" id=\"node1\"><title>139932875254608</title>\n",
       "<polygon fill=\"none\" points=\"14.5,-253 14.5,-299 215.5,-299 215.5,-253 14.5,-253\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"53\" y=\"-272.3\">InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"91.5,-253 91.5,-299 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"119\" y=\"-283.8\">input:</text>\n",
       "<polyline fill=\"none\" points=\"91.5,-276 146.5,-276 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"119\" y=\"-260.8\">output:</text>\n",
       "<polyline fill=\"none\" points=\"146.5,-253 146.5,-299 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"181\" y=\"-283.8\">(None, 4)</text>\n",
       "<polyline fill=\"none\" points=\"146.5,-276 215.5,-276 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"181\" y=\"-260.8\">(None, 4)</text>\n",
       "</g>\n",
       "<!-- 139932875254544 -->\n",
       "<g class=\"node\" id=\"node2\"><title>139932875254544</title>\n",
       "<polygon fill=\"none\" points=\"-1,-169 -1,-215 231,-215 231,-169 -1,-169\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"39\" y=\"-188.3\">Embedding</text>\n",
       "<polyline fill=\"none\" points=\"79,-169 79,-215 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"106.5\" y=\"-199.8\">input:</text>\n",
       "<polyline fill=\"none\" points=\"79,-192 134,-192 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"106.5\" y=\"-176.8\">output:</text>\n",
       "<polyline fill=\"none\" points=\"134,-169 134,-215 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"182.5\" y=\"-199.8\">(None, 4)</text>\n",
       "<polyline fill=\"none\" points=\"134,-192 231,-192 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"182.5\" y=\"-176.8\">(None, 4, 100)</text>\n",
       "</g>\n",
       "<!-- 139932875254608&#45;&gt;139932875254544 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>139932875254608-&gt;139932875254544</title>\n",
       "<path d=\"M115,-252.593C115,-244.118 115,-234.297 115,-225.104\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"118.5,-225.096 115,-215.096 111.5,-225.096 118.5,-225.096\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139932874939856 -->\n",
       "<g class=\"node\" id=\"node3\"><title>139932874939856</title>\n",
       "<polygon fill=\"none\" points=\"8,-85 8,-131 222,-131 222,-85 8,-85\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"39\" y=\"-104.3\">Lambda</text>\n",
       "<polyline fill=\"none\" points=\"70,-85 70,-131 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"97.5\" y=\"-115.8\">input:</text>\n",
       "<polyline fill=\"none\" points=\"70,-108 125,-108 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"97.5\" y=\"-92.8\">output:</text>\n",
       "<polyline fill=\"none\" points=\"125,-85 125,-131 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"173.5\" y=\"-115.8\">(None, 4, 100)</text>\n",
       "<polyline fill=\"none\" points=\"125,-108 222,-108 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"173.5\" y=\"-92.8\">(None, 100)</text>\n",
       "</g>\n",
       "<!-- 139932875254544&#45;&gt;139932874939856 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>139932875254544-&gt;139932874939856</title>\n",
       "<path d=\"M115,-168.593C115,-160.118 115,-150.297 115,-141.104\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"118.5,-141.096 115,-131.096 111.5,-141.096 118.5,-141.096\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139932874937040 -->\n",
       "<g class=\"node\" id=\"node4\"><title>139932874937040</title>\n",
       "<polygon fill=\"none\" points=\"21,-1 21,-47 209,-47 209,-1 21,-1\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"46\" y=\"-20.3\">Dense</text>\n",
       "<polyline fill=\"none\" points=\"71,-1 71,-47 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"98.5\" y=\"-31.8\">input:</text>\n",
       "<polyline fill=\"none\" points=\"71,-24 126,-24 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"98.5\" y=\"-8.8\">output:</text>\n",
       "<polyline fill=\"none\" points=\"126,-1 126,-47 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"167.5\" y=\"-31.8\">(None, 100)</text>\n",
       "<polyline fill=\"none\" points=\"126,-24 209,-24 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"167.5\" y=\"-8.8\">(None, 80)</text>\n",
       "</g>\n",
       "<!-- 139932874939856&#45;&gt;139932874937040 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>139932874939856-&gt;139932874937040</title>\n",
       "<path d=\"M115,-84.5931C115,-76.1177 115,-66.2974 115,-57.104\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"118.5,-57.0958 115,-47.0959 111.5,-57.0959 118.5,-57.0958\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Lambda\n",
    "\n",
    "# build CBOW architecture\n",
    "cbow = Sequential()\n",
    "cbow.add(Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=window_size*2))\n",
    "cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embed_size,)))\n",
    "cbow.add(Dense(vocab_size, activation='softmax'))\n",
    "cbow.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "# view model summary\n",
    "print(cbow.summary())\n",
    "\n",
    "# visualize model structure\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(cbow, show_shapes=True, show_layer_names=False, \n",
    "                 rankdir='TB').create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch:', 100, ' Loss:', 0.070042146927335125)\n",
      "('Epoch:', 200, ' Loss:', 6.0379752838457534e-05)\n",
      "('Epoch:', 300, ' Loss:', 1.5616416931152344e-05)\n",
      "('Epoch:', 400, ' Loss:', 1.5616416931152344e-05)\n",
      "('Epoch:', 500, ' Loss:', 1.5616416931152344e-05)\n",
      "('Epoch:', 600, ' Loss:', 1.5616416931152344e-05)\n",
      "('Epoch:', 700, ' Loss:', 1.5616416931152344e-05)\n",
      "('Epoch:', 800, ' Loss:', 1.5616416931152344e-05)\n",
      "('Epoch:', 900, ' Loss:', 1.5616416931152344e-05)\n"
     ]
    }
   ],
   "source": [
    "loss_acc = []\n",
    "for epoch in range(1, 1000):\n",
    "    loss = 0.\n",
    "    i = 0\n",
    "    for x, y in generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n",
    "        i += 1\n",
    "        loss += cbow.train_on_batch(x, y)\n",
    "        if i % 100000 == 0:\n",
    "            print('Processed {} (context, word) pairs'.format(i))\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch:', epoch, ' Loss:', loss)\n",
    "    loss_acc.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "x = xrange(1,len(loss_acc)+1)\n",
    "plt.plot(x, loss_acc)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79, 100)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>1.266133</td>\n",
       "      <td>0.349702</td>\n",
       "      <td>0.267971</td>\n",
       "      <td>-0.561434</td>\n",
       "      <td>1.188601</td>\n",
       "      <td>0.978859</td>\n",
       "      <td>1.273472</td>\n",
       "      <td>-1.679764</td>\n",
       "      <td>2.117352</td>\n",
       "      <td>0.012177</td>\n",
       "      <td>...</td>\n",
       "      <td>0.967358</td>\n",
       "      <td>1.825161</td>\n",
       "      <td>-1.785416</td>\n",
       "      <td>0.503959</td>\n",
       "      <td>-3.532048</td>\n",
       "      <td>0.685769</td>\n",
       "      <td>1.186739</td>\n",
       "      <td>0.131504</td>\n",
       "      <td>-0.307449</td>\n",
       "      <td>-0.213507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>0.764007</td>\n",
       "      <td>0.092310</td>\n",
       "      <td>-1.018833</td>\n",
       "      <td>-0.124648</td>\n",
       "      <td>0.154430</td>\n",
       "      <td>0.509249</td>\n",
       "      <td>-0.483014</td>\n",
       "      <td>-0.223415</td>\n",
       "      <td>3.750870</td>\n",
       "      <td>1.165278</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.461299</td>\n",
       "      <td>1.628567</td>\n",
       "      <td>0.622636</td>\n",
       "      <td>-1.497438</td>\n",
       "      <td>0.779358</td>\n",
       "      <td>0.462160</td>\n",
       "      <td>4.330284</td>\n",
       "      <td>-0.186747</td>\n",
       "      <td>0.998975</td>\n",
       "      <td>0.209154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>-1.361582</td>\n",
       "      <td>0.193226</td>\n",
       "      <td>1.877105</td>\n",
       "      <td>1.114656</td>\n",
       "      <td>-1.716753</td>\n",
       "      <td>-0.358301</td>\n",
       "      <td>2.157027</td>\n",
       "      <td>1.909365</td>\n",
       "      <td>-1.167614</td>\n",
       "      <td>0.028600</td>\n",
       "      <td>...</td>\n",
       "      <td>0.466472</td>\n",
       "      <td>-0.737329</td>\n",
       "      <td>-1.399071</td>\n",
       "      <td>0.498821</td>\n",
       "      <td>0.667459</td>\n",
       "      <td>0.996844</td>\n",
       "      <td>1.196580</td>\n",
       "      <td>0.299213</td>\n",
       "      <td>-2.160282</td>\n",
       "      <td>0.246660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this</th>\n",
       "      <td>-0.667266</td>\n",
       "      <td>-1.686569</td>\n",
       "      <td>0.180730</td>\n",
       "      <td>-0.596966</td>\n",
       "      <td>0.708851</td>\n",
       "      <td>1.805048</td>\n",
       "      <td>0.457329</td>\n",
       "      <td>-0.318155</td>\n",
       "      <td>2.964148</td>\n",
       "      <td>-0.890817</td>\n",
       "      <td>...</td>\n",
       "      <td>0.584667</td>\n",
       "      <td>-1.775401</td>\n",
       "      <td>-0.587846</td>\n",
       "      <td>-0.298438</td>\n",
       "      <td>-1.923433</td>\n",
       "      <td>2.214738</td>\n",
       "      <td>0.156102</td>\n",
       "      <td>-0.285599</td>\n",
       "      <td>0.725975</td>\n",
       "      <td>1.995872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>corpus</th>\n",
       "      <td>-2.604547</td>\n",
       "      <td>0.773301</td>\n",
       "      <td>1.616210</td>\n",
       "      <td>-0.751094</td>\n",
       "      <td>1.349274</td>\n",
       "      <td>-1.279612</td>\n",
       "      <td>-0.695607</td>\n",
       "      <td>0.071027</td>\n",
       "      <td>-0.122662</td>\n",
       "      <td>0.133631</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.615990</td>\n",
       "      <td>0.292098</td>\n",
       "      <td>0.513560</td>\n",
       "      <td>-0.196050</td>\n",
       "      <td>0.132287</td>\n",
       "      <td>-1.946740</td>\n",
       "      <td>0.894451</td>\n",
       "      <td>-2.315251</td>\n",
       "      <td>-1.206102</td>\n",
       "      <td>-0.355389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6   \\\n",
       "the     1.266133  0.349702  0.267971 -0.561434  1.188601  0.978859  1.273472   \n",
       "a       0.764007  0.092310 -1.018833 -0.124648  0.154430  0.509249 -0.483014   \n",
       "to     -1.361582  0.193226  1.877105  1.114656 -1.716753 -0.358301  2.157027   \n",
       "this   -0.667266 -1.686569  0.180730 -0.596966  0.708851  1.805048  0.457329   \n",
       "corpus -2.604547  0.773301  1.616210 -0.751094  1.349274 -1.279612 -0.695607   \n",
       "\n",
       "              7         8         9     ...           90        91        92  \\\n",
       "the    -1.679764  2.117352  0.012177    ...     0.967358  1.825161 -1.785416   \n",
       "a      -0.223415  3.750870  1.165278    ...    -1.461299  1.628567  0.622636   \n",
       "to      1.909365 -1.167614  0.028600    ...     0.466472 -0.737329 -1.399071   \n",
       "this   -0.318155  2.964148 -0.890817    ...     0.584667 -1.775401 -0.587846   \n",
       "corpus  0.071027 -0.122662  0.133631    ...    -0.615990  0.292098  0.513560   \n",
       "\n",
       "              93        94        95        96        97        98        99  \n",
       "the     0.503959 -3.532048  0.685769  1.186739  0.131504 -0.307449 -0.213507  \n",
       "a      -1.497438  0.779358  0.462160  4.330284 -0.186747  0.998975  0.209154  \n",
       "to      0.498821  0.667459  0.996844  1.196580  0.299213 -2.160282  0.246660  \n",
       "this   -0.298438 -1.923433  2.214738  0.156102 -0.285599  0.725975  1.995872  \n",
       "corpus -0.196050  0.132287 -1.946740  0.894451 -2.315251 -1.206102 -0.355389  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "weights = cbow.get_weights()[0]\n",
    "weights = weights[1:]\n",
    "print(weights.shape)\n",
    "\n",
    "pd.DataFrame(weights, index=list(id2word.values())[1:]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79, 79)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'unsupervised': ['what', 'are', 'give', 'learning', 'means'],\n",
       " 'word2vec': ['models', 'bunch', 'fact', 'do', 'our']}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test similar words\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "# compute pairwise distance matrix\n",
    "distance_matrix = euclidean_distances(weights)\n",
    "print(distance_matrix.shape)\n",
    "\n",
    "# view contextually similar words\n",
    "similar_words = {search_term: [id2word[idx] for idx in distance_matrix[word2id[search_term]-1].argsort()[1:6]+1] \n",
    "                   for search_term in ['word2vec', 'unsupervised']}\n",
    "\n",
    "similar_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# JUST ANOTHER CBOW IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(corpus):\n",
    "    \"\"\"Tokenize the corpus text.\n",
    "    :param corpus: list containing a string of text (example: [\"I like playing football with my friends\"])\n",
    "    :return corpus_tokenized: indexed list of words in the corpus, in the same order as the original corpus (the example above would return [[1, 2, 3, 4]])\n",
    "    :return V: size of vocabulary\n",
    "    \"\"\"\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    corpus_tokenized = tokenizer.texts_to_sequences(corpus)\n",
    "    V = len(tokenizer.word_index)\n",
    "    return corpus_tokenized, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_categorical(y, num_classes=None):\n",
    "    \"\"\"Converts a class vector (integers) to binary class matrix.\n",
    "    E.g. for use with categorical_crossentropy.\n",
    "    # Arguments\n",
    "        y: class vector to be converted into a matrix\n",
    "            (integers from 0 to num_classes).\n",
    "        num_classes: total number of classes.\n",
    "    # Returns\n",
    "        A binary matrix representation of the input.\n",
    "    \"\"\"\n",
    "    y = np.array(y, dtype='int')\n",
    "    input_shape = y.shape\n",
    "    if input_shape and input_shape[-1] == 1 and len(input_shape) > 1:\n",
    "        input_shape = tuple(input_shape[:-1])\n",
    "    y = y.ravel()\n",
    "    if not num_classes:\n",
    "        num_classes = np.max(y) + 1\n",
    "    n = y.shape[0]\n",
    "    categorical = np.zeros((n, num_classes))\n",
    "    categorical[np.arange(n), y] = 1\n",
    "    output_shape = input_shape + (num_classes,)\n",
    "    categorical = np.reshape(categorical, output_shape)\n",
    "    return categorical\n",
    "\n",
    "def corpus2io(corpus_tokenized, V, window_size):\n",
    "    \"\"\"Converts corpus text into context and center words\n",
    "    # Arguments\n",
    "        corpus_tokenized: corpus text\n",
    "        window_size: size of context window\n",
    "    # Returns\n",
    "        context and center words (arrays)\n",
    "    \"\"\"\n",
    "    for words in corpus_tokenized:\n",
    "        L = len(words)\n",
    "        for index, word in enumerate(words):\n",
    "            contexts = []\n",
    "            labels = []\n",
    "            s = index - window_size\n",
    "            e = index + window_size + 1\n",
    "            contexts.append([words[i]-1 for i in range(s, e) if 0 <= i < L and i != index])\n",
    "            labels.append(word-1)\n",
    "            x = np_utils.to_categorical(contexts, V)\n",
    "            y = np_utils.to_categorical(labels, V)\n",
    "            yield (x, y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "('center word =', array([ 1.,  0.,  0.,  0.,  0.,  0.,  0.]))\n",
      "context words =\n",
      "[[ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.]]\n",
      "1\n",
      "('center word =', array([ 0.,  1.,  0.,  0.,  0.,  0.,  0.]))\n",
      "context words =\n",
      "[[ 1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.]]\n",
      "2\n",
      "('center word =', array([ 0.,  0.,  1.,  0.,  0.,  0.,  0.]))\n",
      "context words =\n",
      "[[ 1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.]]\n",
      "3\n",
      "('center word =', array([ 0.,  0.,  0.,  1.,  0.,  0.,  0.]))\n",
      "context words =\n",
      "[[ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.]]\n",
      "4\n",
      "('center word =', array([ 0.,  0.,  0.,  0.,  1.,  0.,  0.]))\n",
      "context words =\n",
      "[[ 0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.]]\n",
      "5\n",
      "('center word =', array([ 0.,  0.,  0.,  0.,  0.,  1.,  0.]))\n",
      "context words =\n",
      "[[ 0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.]]\n",
      "6\n",
      "('center word =', array([ 0.,  0.,  0.,  0.,  0.,  0.,  1.]))\n",
      "context words =\n",
      "[[ 0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "window_size = 2\n",
    "corpus = [\"I like playing football with my friends\"]\n",
    "corpus_tokenized, V = tokenize(corpus)\n",
    "for i, (x, y) in enumerate(corpus2io(corpus_tokenized, V, window_size)):\n",
    "    print(i)\n",
    "    print(\"center word =\", y)\n",
    "    print(\"context words =\")\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Calculate softmax based probability for given input vector\n",
    "    # Arguments\n",
    "        x: numpy array/list\n",
    "    # Returns\n",
    "        softmax of input array\n",
    "    \"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cbow(context, label, W1, W2, loss):\n",
    "    \"\"\"\n",
    "    Implementation of Continuous-Bag-of-Words Word2Vec model\n",
    "    :param context: all the context words (these represent the inputs)\n",
    "    :param label: the center word (this represents the label)\n",
    "    :param W1: weights from the input to the hidden layer\n",
    "    :param W2: weights from the hidden to the output layer\n",
    "    :param loss: float that represents the current value of the loss function\n",
    "    :return: updated weights and loss\n",
    "    \"\"\"\n",
    "    x = np.mean(context, axis=0)\n",
    "    print(W1.shape, x.shape)\n",
    "    h = np.dot(W1.T, x)\n",
    "    print(h)\n",
    "    u = np.dot(W2.T, h)\n",
    "    y_pred = softmax(u)\n",
    "\n",
    "    e = -label + y_pred\n",
    "    dW2 = np.outer(h, e)\n",
    "    dW1 = np.outer(x, np.dot(W2, e))\n",
    "\n",
    "    new_W1 = W1 - eta * dW1\n",
    "    new_W2 = W2 - eta * dW2\n",
    "\n",
    "    loss += -float(u[label == 1]) + np.log(np.sum(np.exp(u)))\n",
    "\n",
    "    return new_W1, new_W2, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((7, 2), (7,))\n",
      "[ 0.21461822  0.48317263]\n",
      "Training example #0 \n",
      "-------------------- \n",
      "\n",
      " \t label = [ 1.  0.  0.  0.  0.  0.  0.], \n",
      " \t context = [[ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.]]\n",
      "\t W1 = [[ 0.54340494  0.27836939]\n",
      " [ 0.40739238  0.86832668]\n",
      " [-0.01240636  0.14511967]\n",
      " [ 0.67074908  0.82585276]\n",
      " [ 0.13670659  0.57509333]\n",
      " [ 0.89132195  0.20920212]\n",
      " [ 0.18532822  0.10837689]]\n",
      "\t W2 = [[  2.37522044e-01   9.74588882e-01   8.08598345e-01   1.69452859e-01\n",
      "    8.13081663e-01   2.71730692e-01   4.28973634e-01]\n",
      " [  9.80158449e-01   8.08565553e-01   3.29167094e-01   1.69808842e-01\n",
      "    3.65755980e-01   4.13558689e-04   2.46279033e-01]] \n",
      "\t loss = 1.77504194359\n",
      "\n",
      "((7, 2), (7,))\n",
      "[ 0.40058256  0.41644727]\n",
      "Training example #1 \n",
      "-------------------- \n",
      "\n",
      " \t label = [ 0.  1.  0.  0.  0.  0.  0.], \n",
      " \t context = [[ 1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.]]\n",
      "\t W1 = [[  5.56639604e-01   2.89686158e-01]\n",
      " [  4.07392378e-01   8.68326677e-01]\n",
      " [  8.28305455e-04   1.56436438e-01]\n",
      " [  6.83983747e-01   8.37169528e-01]\n",
      " [  1.36706590e-01   5.75093329e-01]\n",
      " [  8.91321954e-01   2.09202122e-01]\n",
      " [  1.85328220e-01   1.08376890e-01]]\n",
      "\t W2 = [[ 0.23121195  1.00675437  0.80254985  0.16507125  0.80692926  0.26747677\n",
      "   0.42395468]\n",
      " [ 0.97359845  0.84200492  0.32287905  0.16525371  0.35935991 -0.00400884\n",
      "   0.24106131]] \n",
      "\t loss = 3.3994293405\n",
      "\n",
      "((7, 2), (7,))\n",
      "[ 0.44618058  0.64256892]\n",
      "Training example #2 \n",
      "-------------------- \n",
      "\n",
      " \t label = [ 0.  0.  1.  0.  0.  0.  0.], \n",
      " \t context = [[ 1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.]]\n",
      "\t W1 = [[  5.61872952e-01   2.85249823e-01]\n",
      " [  4.12625726e-01   8.63890342e-01]\n",
      " [  8.28305455e-04   1.56436438e-01]\n",
      " [  6.89217095e-01   8.32733193e-01]\n",
      " [  1.41939938e-01   5.70656995e-01]\n",
      " [  8.91321954e-01   2.09202122e-01]\n",
      " [  1.85328220e-01   1.08376890e-01]]\n",
      "\t W2 = [[ 0.22354561  0.99679699  0.84065603  0.16064342  0.80024989  0.26331958\n",
      "   0.41873659]\n",
      " [ 0.96255774  0.82766477  0.37775784  0.15887694  0.3497406  -0.00999584\n",
      "   0.23354646]] \n",
      "\t loss = 5.32394118748\n",
      "\n",
      "((7, 2), (7,))\n",
      "[ 0.36167898  0.45004647]\n",
      "Training example #3 \n",
      "-------------------- \n",
      "\n",
      " \t label = [ 0.  0.  0.  1.  0.  0.  0.], \n",
      " \t context = [[ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.]]\n",
      "\t W1 = [[ 0.56187295  0.28524982]\n",
      " [ 0.40213707  0.85600117]\n",
      " [-0.00966035  0.14854727]\n",
      " [ 0.68921709  0.83273319]\n",
      " [ 0.13145128  0.56276783]\n",
      " [ 0.88083329  0.20131295]\n",
      " [ 0.18532822  0.10837689]]\n",
      "\t W2 = [[ 0.21775807  0.98959276  0.83509537  0.19287099  0.79483857  0.25952945\n",
      "   0.4142629 ]\n",
      " [ 0.95535615  0.81870036  0.37083857  0.19897854  0.34300715 -0.01471198\n",
      "   0.22797972]] \n",
      "\t loss = 7.54084957869\n",
      "\n",
      "((7, 2), (7,))\n",
      "[ 0.43642956  0.32274258]\n",
      "Training example #4 \n",
      "-------------------- \n",
      "\n",
      " \t label = [ 0.  0.  0.  0.  1.  0.  0.], \n",
      " \t context = [[ 0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.]]\n",
      "\t W1 = [[ 0.56187295  0.28524982]\n",
      " [ 0.40213707  0.85600117]\n",
      " [-0.00432069  0.14562168]\n",
      " [ 0.69455676  0.82980761]\n",
      " [ 0.13145128  0.56276783]\n",
      " [ 0.88617296  0.19838737]\n",
      " [ 0.19066788  0.1054513 ]]\n",
      "\t W2 = [[ 0.21139714  0.98106847  0.82819932  0.18794169  0.83176617  0.25479284\n",
      "   0.40878249]\n",
      " [ 0.95065221  0.81239659  0.36573889  0.19533328  0.37031536 -0.01821474\n",
      "   0.22392692]] \n",
      "\t loss = 9.41249372653\n",
      "\n",
      "((7, 2), (7,))\n",
      "[ 0.33889197  0.49934225]\n",
      "Training example #5 \n",
      "-------------------- \n",
      "\n",
      " \t label = [ 0.  0.  0.  0.  0.  1.  0.], \n",
      " \t context = [[ 0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.]]\n",
      "\t W1 = [[ 0.56187295  0.28524982]\n",
      " [ 0.40213707  0.85600117]\n",
      " [-0.00432069  0.14562168]\n",
      " [ 0.68382212  0.81335847]\n",
      " [ 0.12071664  0.54631869]\n",
      " [ 0.88617296  0.19838737]\n",
      " [ 0.17993324  0.08900217]]\n",
      "\t W2 = [[ 0.2058461   0.97434378  0.82309063  0.18416489  0.82663959  0.28520946\n",
      "   0.40465366]\n",
      " [ 0.94247299  0.80248806  0.35821146  0.18976835  0.36276158  0.0266028\n",
      "   0.21784328]] \n",
      "\t loss = 11.6906933526\n",
      "\n",
      "((7, 2), (7,))\n",
      "[ 0.5034448   0.37235303]\n",
      "Training example #6 \n",
      "-------------------- \n",
      "\n",
      " \t label = [ 0.  0.  0.  0.  0.  0.  1.], \n",
      " \t context = [[ 0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.]]\n",
      "\t W1 = [[ 0.56187295  0.28524982]\n",
      " [ 0.40213707  0.85600117]\n",
      " [-0.00432069  0.14562168]\n",
      " [ 0.68382212  0.81335847]\n",
      " [ 0.11156295  0.53410073]\n",
      " [ 0.87701927  0.18616941]\n",
      " [ 0.17993324  0.08900217]]\n",
      "\t W2 = [[ 0.19857939  0.96418767  0.81511412  0.17873393  0.81863527  0.27983192\n",
      "   0.44886582]\n",
      " [ 0.93709845  0.7949765   0.35231195  0.18575155  0.3568415   0.02262551\n",
      "   0.25054305]] \n",
      "\t loss = 13.7960082689\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#user-defined parameters\n",
    "corpus = [\"I like playing football with my friends\"] #our example text corpus\n",
    "N = 2 #assume that the hidden layer has dimensionality = 2\n",
    "window_size = 2 #symmetrical\n",
    "eta = 0.1 #learning rate\n",
    "\n",
    "corpus_tokenized, V = tokenize(corpus)\n",
    "\n",
    "#initialize weights (with random values) and loss function\n",
    "np.random.seed(100)\n",
    "W1 = np.random.rand(V, N)\n",
    "W2 = np.random.rand(N, V)\n",
    "loss = 0.\n",
    "\n",
    "for i, (context, label) in enumerate(corpus2io(corpus_tokenized, V, window_size)):\n",
    "    W1, W2, loss = cbow(context, label, W1, W2, loss)\n",
    "    print(\"Training example #{} \\n-------------------- \\n\\n \\t label = {}, \\n \\t context = {}\".format(i, label, context))\n",
    "    print(\"\\t W1 = {}\\n\\t W2 = {} \\n\\t loss = {}\\n\".format(W1, W2, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'h' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-01fbdc44ef81>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mh\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'h' is not defined"
     ]
    }
   ],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
